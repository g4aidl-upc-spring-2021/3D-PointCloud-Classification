{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final PointNet document.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXkCWWsrtvhX"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuAOf6rczlR8"
      },
      "source": [
        "## Check GPU\n",
        "In order to perform the experiments in a reasonable time, check whether the GPU has at least 15GiB. If it is not, it's necessary to restart the runtime until this requirement is satisfied"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtkYCXLDtnFy"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IW_7Ofa8uJ2N"
      },
      "source": [
        "## Installations and imports\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_Lhkgjszi9e"
      },
      "source": [
        "### Installations\n",
        "As some libraries that are not in the default version in colab are used, it is necessary to install them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJJ_WspuuHl4"
      },
      "source": [
        "!pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html\n",
        "!pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html\n",
        "!pip install torch-cluster -f https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html\n",
        "!pip install torch-geometric -f https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html\n",
        "\n",
        "!pip install torchmetrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMkOkC8pua4M"
      },
      "source": [
        "### Imports\n",
        "In the next snippet of code there are all the imports necessaries for the project and the tensorboard is initialized."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_hW0nahumSt"
      },
      "source": [
        "import os\n",
        "import datetime\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow\n",
        "import tensorboard\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "from torchmetrics import Accuracy\n",
        "\n",
        "from torch_geometric.data import DataLoader\n",
        "from torch_geometric.utils import to_dense_batch\n",
        "from torch_geometric.datasets import ModelNet\n",
        "from torch_geometric.transforms import SamplePoints, NormalizeScale, RandomFlip, RandomRotate, Compose\n",
        "\n",
        "%reload_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8ES3WU2vlBt"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncrMD2XZvxXn"
      },
      "source": [
        "hparams = {\n",
        "    'bs': 32,\n",
        "    'epochs': 100,\n",
        "    'device': torch.device('cuda:0' if torch.cuda.is_available() else 'cpu'),\n",
        "    'tb_logs': '/content/drive/MyDrive/Adapt',\n",
        "    'tb_name': 'tb_' + str(datetime.datetime.utcnow()),\n",
        "    'drive_root': '/content/drive/MyDrive/Dataset/',\n",
        "    'normalize_scale': True, \n",
        "    'data_augmentation': 'flip_rotate',\n",
        "    'fixed_num_of_points': 1024,\n",
        "    'flip_probability': 0.5,\n",
        "    'flip_axis': 1,\n",
        "    'rotate_degrees': 45,\n",
        "    'rotate_axis': 0,\n",
        "    'model_log': '/content/drive/MyDrive/Adapt/', \n",
        "    'model_name': 'pointNet_flip_rotate_0.3_adam_OneCycleLR',\n",
        "    'k': 3,\n",
        "    'num_classes': 10,\n",
        "    'dropout': 0.3,\n",
        "    'optimizer': 'Adam',\n",
        "    'lr': 1e-3,\n",
        "    'wd': 1e-3,\n",
        "    'momentum': 0.9,\n",
        "    'scheduler': 'OneCycleLR',\n",
        "    'gamma': 0.5,\n",
        "    'patience': 10,\n",
        "    'step_size': 20\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfTHZ9mwv6xE"
      },
      "source": [
        "## Seeds "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUnHQOUJv898"
      },
      "source": [
        "seed = 42\n",
        "# Controlling sources of randomness\n",
        "torch.manual_seed(seed) # generate random numbers for all devices (both CPU and CUDA)\n",
        "# Random number generators in other libraries:\n",
        "np.random.seed(seed)\n",
        "# CUDA convolution benchmarking:\n",
        "torch.backends.cudnn.benchmark = False # ensures that CUDA selects the same algorithm each time an application is run"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1o14VlVAwDoz"
      },
      "source": [
        "# Model - PointNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsjZajvrwGJE"
      },
      "source": [
        "class TNet(nn.Module):\n",
        "  def __init__(self, k=3):\n",
        "    super().__init__()\n",
        "    self.k = k\n",
        "    \n",
        "    self.Conv1 = nn.Conv1d(in_channels=k, out_channels=64, kernel_size=1)\n",
        "    self.bn1 = nn.BatchNorm1d(64) \n",
        "    self.Conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=1)\n",
        "    self.bn2 = nn.BatchNorm1d(128)\n",
        "    self.Conv3 = nn.Conv1d(in_channels=128, out_channels=1024, kernel_size=1)\n",
        "    self.bn3 = nn.BatchNorm1d(1024)\n",
        "\n",
        "    self.FC1 = nn.Linear(in_features=1024, out_features=512)\n",
        "    self.bn4 = nn.BatchNorm1d(512)\n",
        "    self.FC2 = nn.Linear(in_features=512, out_features=256)\n",
        "    self.bn5 = nn.BatchNorm1d(256)\n",
        "\n",
        "    self.FC3 = nn.Linear(in_features=256, out_features=k*k)\n",
        "\n",
        "  def forward(self, cloud_points):\n",
        "    bs = cloud_points.size(0)\n",
        "\n",
        "    x = F.relu(self.bn1(self.Conv1(cloud_points)))\n",
        "    x = F.relu(self.bn2(self.Conv2(x)))\n",
        "    x = F.relu(self.bn3(self.Conv3(x)))\n",
        "\n",
        "    # size: [batch size, 1024, # of points]\n",
        "    x = nn.MaxPool1d(x.size(-1))(x) # pool with kernel = # of points/batch\n",
        "    # size: [batch size, 1024, 1]\n",
        "    x = x.view(bs,-1) # flatten to get horizontal vector\n",
        "\n",
        "    # size: [batch size, 1024]\n",
        "    x = F.relu(self.bn4(self.FC1(x)))\n",
        "    x = F.relu(self.bn5(self.FC2(x)))\n",
        "\n",
        "    # diagonal matrices initialized, as many as batch size\n",
        "    init_matrix = torch.eye(self.k, requires_grad=True).repeat(bs,1,1)\n",
        "    if x.is_cuda:\n",
        "      init_matrix = init_matrix.cuda() # gets updated according to f.c. output\n",
        "    matrix = self.FC3(x).view(-1, self.k, self.k) + init_matrix\n",
        "    return matrix\n",
        "\n",
        "class Transform(nn.Module):\n",
        "   def __init__(self, k=3):\n",
        "        super().__init__()\n",
        "        self.input_transform = TNet(k)\n",
        "        self.feature_transform = TNet(k=64)\n",
        "\n",
        "        self.Conv1 = nn.Conv1d(in_channels=3, out_channels=64, kernel_size=1)\n",
        "        self.Conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=1)\n",
        "        self.Conv3 = nn.Conv1d(in_channels=128, out_channels=1024, kernel_size=1)\n",
        "\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.bn3 = nn.BatchNorm1d(1024)\n",
        "\n",
        "   def forward(self, x):\n",
        "        bs = x.size(0)\n",
        "\n",
        "        matrix3x3 = self.input_transform(x)\n",
        "        x = torch.bmm(torch.transpose(x,1,2),matrix3x3).transpose(1,2)\n",
        "        x = F.relu(self.bn1(self.Conv1(x)))\n",
        "        \n",
        "        matrix64x64 = self.feature_transform(x)\n",
        "        x = torch.bmm(torch.transpose(x,1,2), matrix64x64).transpose(1,2) \n",
        "        x = F.relu(self.bn2(self.Conv2(x)))\n",
        "        x = self.bn3(self.Conv3(x))\n",
        "\n",
        "        x = nn.MaxPool1d(x.size(-1))(x)\n",
        "        global_features = x.view(bs,-1)\n",
        "        return global_features\n",
        "\n",
        "class PointNetModel(nn.Module):\n",
        "    def __init__(self, k=3, num_classes=16, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.transform = Transform(k)\n",
        "\n",
        "        self.FC1 = nn.Linear(in_features=1024, out_features=512)\n",
        "        self.bn1 = nn.BatchNorm1d(512)\n",
        "        self.FC2 = nn.Linear(in_features=512, out_features=256)\n",
        "        self.bn2 = nn.BatchNorm1d(256)\n",
        "        self.FC3 = nn.Linear(in_features=256, out_features=num_classes)\n",
        "        self.dropout = nn.Dropout(p=0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        global_features = self.transform(x)\n",
        "        x = F.relu(self.bn1(self.FC1(global_features)))\n",
        "        x = self.FC2(x)\n",
        "        # apply dropout if exists\n",
        "        x = self.dropout(x) if self.dropout is not None else x\n",
        "        x = F.relu(self.bn2(x))\n",
        "        output = self.FC3(x)\n",
        "        return output, F.softmax(output,dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMop5xGPwO05"
      },
      "source": [
        "# Dataset\n",
        "First of all, it is necessary to make the drive folder with the dataset available to this collab in order not to download it every time. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nimVsjqRwdeM",
        "outputId": "b1eabac0-1d7d-46f6-ac1a-614803779e83"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-9UhaL9whH0"
      },
      "source": [
        "## Transformations\n",
        "In this project is necessary to use some transformations to either normalize the data or to perform data augmentation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1PbuRkFwgAk"
      },
      "source": [
        "def get_pre_transformation(number_points=1024):\n",
        "    return SamplePoints(num=number_points)\n",
        "\n",
        "def get_transformation(normalize_scale):\n",
        "    if normalize_scale:  \n",
        "      return NormalizeScale()\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def get_random_flip(axis=1, p=0.5):\n",
        "    return RandomFlip(axis, p)\n",
        "\n",
        "\n",
        "def get_random_rotation(degrees=45, axis=1):\n",
        "    return RandomRotate(degrees, axis)\n",
        "\n",
        "\n",
        "def data_augmentation_flip(normalize_scale, axis=1, p=0.5):\n",
        "    return Compose([get_transformation(normalize_scale), get_random_flip(axis, p)])\n",
        "\n",
        "\n",
        "def data_augmentation_rotation(normalize_scale, axis=1, degrees=45):\n",
        "    return Compose([get_transformation(normalize_scale), get_random_rotation(axis=axis, degrees=degrees)])\n",
        "\n",
        "\n",
        "def data_augmentation_flip_rotation(normalize_scale, axis_flip=1, p=0.5, axis_rotation=1, degrees=45):\n",
        "    return Compose([get_transformation(normalize_scale), get_random_flip(axis_flip, p),\n",
        "                    get_random_rotation(axis=axis_rotation, degrees=degrees)])\n",
        "\n",
        "\n",
        "def get_data_augmentation(dataset, transformation, normalize_scale, axis_flip=1, p=0.5, axis_rotation=1, degrees=45):\n",
        "  if transformation is not None:\n",
        "    if transformation.lower() == 'flip_rotation':\n",
        "        dataset.transform = data_augmentation_flip_rotation(normalize_scale, axis_flip, p, axis_rotation, degrees)\n",
        "    elif transformation.lower() == 'flip':\n",
        "        dataset.transform = data_augmentation_flip(normalize_scale, axis=axis_flip, p=p)\n",
        "    elif transformation.lower() == 'rotate':\n",
        "        dataset.transform = data_augmentation_rotation(normalize_scale, axis=axis_rotation, degrees=degrees)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOIlQzFAwyRk"
      },
      "source": [
        "## Training, validation and test data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jUY_tEXzdyk"
      },
      "source": [
        "In this step, some processing of the data is going to be used to be able to feed it to the model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmvjpqwHxAlB"
      },
      "source": [
        "def get_dataset(root, transform, pre_transform):\n",
        "    train_valid_dataset = ModelNet(root=root, name=\"10\", train=True, pre_transform=pre_transform, transform=transform)\n",
        "    test_dataset = ModelNet(root=root, name=\"10\", train=False, pre_transform=pre_transform, transform=transform)\n",
        "    return train_valid_dataset, test_dataset\n",
        "\n",
        "# Function used to split between validation and training\n",
        "def get_split(index_file_root, dataset):\n",
        "    index_file = open(index_file_root, 'r')\n",
        "    train_index = []\n",
        "    for idx in index_file:\n",
        "        train_index.append(int(idx))\n",
        "\n",
        "    return dataset[train_index]\n",
        "\n",
        "# Function to decide which points are used in validation and which ones in training\n",
        "def create_file_if_necessary(train_file, valid_file, dataset):\n",
        "    if not os.path.isfile(train_file) and not os.path.isfile(valid_file):\n",
        "        torch.manual_seed(0)\n",
        "        # Shuffle before splitting data (random split)\n",
        "        _, perm = dataset.shuffle(return_perm=True)\n",
        "\n",
        "        # Create two files with the indices od the training and validation data\n",
        "        train_idx = open(train_file, 'w+')\n",
        "        valid_idx = open(valid_file, 'w+')\n",
        "\n",
        "        # Split the tensor of indices in training and validation\n",
        "        train_split, val_split = perm.split(round(len(perm) * 0.8))\n",
        "\n",
        "        for i in range(len(train_split)):\n",
        "            train_idx.writelines(str(train_split[i].item()) + \"\\n\")\n",
        "        for i in range(len(val_split)):\n",
        "            valid_idx.writelines(str(val_split[i].item()) + \"\\n\")\n",
        "        \n",
        "        print(\"New split file has been created\")\n",
        "\n",
        "        train_idx.close()\n",
        "        valid_idx.close()\n",
        "\n",
        "    elif not os.path.isfile(train_file) or not os.path.isfile(valid_file):\n",
        "        raise ValueError('One file exists and the other one does not')\n",
        "\n",
        "# Function to be called when creating dataset \n",
        "def get_train_valid_test_ModelNet(root, number_points=1024, normalize_scale=True):\n",
        "    dataset_root = os.path.join(root, 'ModelNet')\n",
        "    train_valid_split, test_split = get_dataset(dataset_root, transform=NormalizeScale() if normalize_scale else None,\n",
        "                                                pre_transform=SamplePoints(num=number_points))\n",
        "\n",
        "    train_split_root = os.path.join(root, 'train_split.txt')\n",
        "    valid_split_root = os.path.join(root, 'val_split.txt')\n",
        "    create_file_if_necessary(train_split_root, valid_split_root, train_valid_split)\n",
        "\n",
        "    train_split = get_split(index_file_root=train_split_root, dataset=train_valid_split)\n",
        "    valid_split = get_split(index_file_root=valid_split_root, dataset=train_valid_split)\n",
        "    return train_split, valid_split, test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7LaMP_oxhEG"
      },
      "source": [
        "# Helper functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xq91dl5Lzabu"
      },
      "source": [
        "As there are some functionalities that are used by different functions or can be used in the future, a list of helpers fucntions has been created"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPM0ugw7xv5E"
      },
      "source": [
        "# Initialize the tensorflow\n",
        "def get_tensorboard_writer(root):\n",
        "    tensorflow.io.gfile = tensorboard.compat.tensorflow_stub.io.gfile  # avoid tensorboard crash when adding embeddings\n",
        "    train_log_dir = os.path.join(root, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"), 'train')\n",
        "    valid_log_dir = os.path.join(root, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"), 'valid')\n",
        "    train_writer = SummaryWriter(log_dir=train_log_dir)\n",
        "    valid_writer = SummaryWriter(log_dir=valid_log_dir)\n",
        "    return train_writer, valid_writer\n",
        "\n",
        "# Writes information of every epoch in the tensorboard\n",
        "def write_epoch_data(train_writer, valid_writer, train_loss, valid_loss, train_accuracy, valid_accuracy, epoch):\n",
        "    # Write Loss and Accuracy in tensorboard:\n",
        "    train_writer.add_scalar('Loss', train_loss, epoch)\n",
        "    train_writer.add_scalar('Accu', train_accuracy, epoch)\n",
        "    valid_writer.add_scalar('Loss', valid_loss, epoch)\n",
        "    valid_writer.add_scalar('Accu', valid_accuracy, epoch)\n",
        "\n",
        "# Funtion to save the best model so far\n",
        "def update_best_model(valid_accuracy, model_state_dict, model_root, model_name):\n",
        "    model_path = os.path.join(model_root, model_name + datetime.datetime.now().strftime(\"%Y%m%d%h\"))\n",
        "    torch.save(model_state_dict, model_path + '.pt')\n",
        "    return valid_accuracy, model_path\n",
        "\n",
        "# Method to visualize a cloud point\n",
        "def visualize_point_cloud(point_cloud):\n",
        "    points, y = point_cloud\n",
        "    fig = go.Figure(data=[go.Mesh3d(x=points[1][:, 0], y=points[1][:, 1], z=points[1][:, 2], mode='markers', marker=dict(size=3, opacity=1))])\n",
        "    fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbkT9nktxUS0"
      },
      "source": [
        "# Training and testing functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4ic2rkAzW08"
      },
      "source": [
        "## Correct parameters\n",
        "As different experiments will be performed some variables need to be dependent on hyperparameters. That is why some functions have been used due to not modify the code when performing different experiments. Adding new conditions to the if - else statement, more schedulers or optimizers can be added."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhcVsCBhxZrc"
      },
      "source": [
        "def get_optimizer(optimizer_name, model_parameters, lr, wd, momentum):\n",
        "    if optimizer_name.lower() == \"Adam\".lower():\n",
        "        return torch.optim.Adam(model_parameters, lr=lr, weight_decay=wd)\n",
        "    elif optimizer_name.upper() == \"SGD\":\n",
        "        return torch.optim.SGD(model_parameters, lr=lr, momentum=momentum)\n",
        "    else:\n",
        "        raise ValueError('Optimizer is not correctly introduced')\n",
        "\n",
        "def get_scheduler(scheduler_name, optimizer, lr=1e-3, gamma=0.5, patience=10, step_size=20, train_loader_len=1024,\n",
        "                  num_epochs=100):\n",
        "  if scheduler_name is not None:\n",
        "    if scheduler_name.lower() == 'StepLR'.lower():\n",
        "        return torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma), None, False\n",
        "    elif scheduler_name.lower() == 'ReduceLROnPlateau'.lower():\n",
        "        return torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=gamma, patience=patience), None, True\n",
        "    elif scheduler_name.lower() == 'OneCycleLR'.lower():\n",
        "        scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, lr, steps_per_epoch=train_loader_len,\n",
        "                                                        epochs=num_epochs)\n",
        "        return scheduler, scheduler, None\n",
        "    else:\n",
        "      raise ValueError('Incorrect scheduler')\n",
        "  else:\n",
        "    print('No scheduler')\n",
        "    return None, None, None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMifOI3kzM0d"
      },
      "source": [
        "## Training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVbOedv_zQWF"
      },
      "source": [
        "# Train One Epoch:\n",
        "def train_epoch(model, train_loader, optimizer, criterion, accuracy, device, scheduler):\n",
        "    # Model in train mode:\n",
        "    model.train()\n",
        "    # List for epoch loss:\n",
        "    epoch_train_loss = []\n",
        "    # Metric stored information reset:\n",
        "    accuracy.reset()\n",
        "\n",
        "    # Train epoch loop:\n",
        "    for i, data in enumerate(train_loader, 1):\n",
        "        # Data retrieval from each bath:\n",
        "        points = to_dense_batch(data.pos, batch=data.batch)[0].to(device).float().transpose(1, 2)\n",
        "        targets = data.y.to(device)\n",
        "\n",
        "        # Forward pass:\n",
        "        preds, probs = model(points)\n",
        "        # Loss calculation + Backpropagation pass\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(preds.to(device), targets)\n",
        "        epoch_train_loss.append(loss.item())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Step for OneCycle scheduler\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        # Batch metrics calculation:\n",
        "        accuracy.update(probs, targets)\n",
        "\n",
        "    # Mean epoch metrics calculation:\n",
        "    mean_loss = np.mean(epoch_train_loss)\n",
        "    mean_accu = accuracy.compute().item()\n",
        "    # Print of all metrics:\n",
        "    print('Train loss: ', mean_loss, \"| Acc.: \" , mean_accu)\n",
        "    return mean_loss, mean_accu\n",
        "\n",
        "\n",
        "# Valid One Epoch:\n",
        "def valid_epoch(model, valid_loader, criterion, accuracy, device):\n",
        "    # Model in validation (evaluation) mode:\n",
        "    model.eval()\n",
        "    # List for epoch loss:\n",
        "    epoch_valid_loss = []\n",
        "    # Metric stored information reset:\n",
        "    accuracy.reset()\n",
        "    # Batch loop for validation:\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(valid_loader, 1):\n",
        "            # Data retrieval from each bath:\n",
        "            points = to_dense_batch(data.pos, batch=data.batch)[0].to(device).float().transpose(1, 2)\n",
        "            targets = data.y.to(device)\n",
        "\n",
        "            # Forward pass:\n",
        "            preds, probs = model(points)\n",
        "            # Loss calculation\n",
        "            loss = criterion(preds.to(device), targets)\n",
        "            epoch_valid_loss.append(loss.item())\n",
        "            # Batch metrics calculation:\n",
        "            accuracy.update(probs, targets)\n",
        "    # Mean epoch metrics calculation:\n",
        "    mean_loss = np.mean(epoch_valid_loss)\n",
        "    mean_accu = accuracy.compute().item()\n",
        "    # Print of all metrics:\n",
        "    print('Valid loss: ', mean_loss, \"| Acc.: \", mean_accu)\n",
        "    return mean_loss, mean_accu\n",
        "\n",
        "\n",
        "def fit(train_data, valid_data, num_classes, k=3, bs=32, num_epochs=100, lr=1e-3):\n",
        "    # Data Loaders for train and validation:\n",
        "    train_loader = DataLoader(train_data, batch_size=bs, shuffle=True)\n",
        "    valid_loader = DataLoader(valid_data, batch_size=bs, shuffle=False)\n",
        "\n",
        "    # Obtain correct model\n",
        "    model = PointNetModel(k, num_classes, hparams['dropout']).to(hparams['device'])\n",
        "\n",
        "    optimizer = get_optimizer(hparams['optimizer'], model.parameters(), lr, hparams['wd'], hparams['momentum'])\n",
        "\n",
        "    # Obtain correct scheduler: train scheduler is used to determine wheteher or not there is a scheduler in the train fucntion,\n",
        "    # the fit scheduler can have three values: None (no scheduler in fit function), True (needs the valid loss parameter), \n",
        "    # False(no need of extra parameter)\n",
        "    scheduler, train_scheduler, fit_scheduler = get_scheduler(hparams['scheduler'], optimizer, lr, hparams['gamma'],\n",
        "                                                              hparams['patience'], hparams['step_size'],\n",
        "                                                              len(train_loader),\n",
        "                                                              num_epochs)\n",
        "    criterion = nn.CrossEntropyLoss().to(hparams['device'])\n",
        "\n",
        "    # Metric\n",
        "    accuracy = Accuracy(average='micro', compute_on_step=False).to(hparams['device'])\n",
        "\n",
        "    # Tensorboard set up\n",
        "    train_writer, valid_writer = get_tensorboard_writer(hparams['tb_logs'])\n",
        "\n",
        "    # Minimum accuracy to save the model\n",
        "    best_accuracy = 0.0\n",
        "    model_root = None\n",
        "    print('Start training...')\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        print('Epoch: ', epoch)\n",
        "        # Train and validation\n",
        "        train_loss, train_accu = train_epoch(model, train_loader, optimizer, criterion, accuracy, hparams['device'],\n",
        "                                             train_scheduler)\n",
        "        valid_loss, valid_accu = valid_epoch(model, valid_loader, criterion, accuracy, hparams['device'])\n",
        "\n",
        "        # Choose scheduler\n",
        "        if fit_scheduler is not None:\n",
        "          if fit_scheduler:\n",
        "            scheduler.step(valid_loss) \n",
        "          else: \n",
        "            scheduler.step()\n",
        "\n",
        "        write_epoch_data(train_writer, valid_writer, train_loss, valid_loss, train_accu, valid_accu, epoch)\n",
        "\n",
        "        # Save best model:\n",
        "        if best_accuracy < valid_accu:\n",
        "            best_accuracy, model_root = update_best_model(valid_accu, model.state_dict(), hparams['model_log'], hparams['model_name'])\n",
        "\n",
        "    final_state_dict_root = model_root + '.pt'\n",
        "    model.load_state_dict(torch.load(final_state_dict_root))\n",
        "    print(\"Best val accuracy: \", best_accuracy)\n",
        "    return best_accuracy, final_state_dict_root"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_MA21fX1NQO"
      },
      "source": [
        "## Test inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPtA3HPO1RMD"
      },
      "source": [
        "def test(test_data, model_state_dict_root):\n",
        "    test_loader = DataLoader(test_data, batch_size=1, shuffle=False)\n",
        "    model = PointNetModel(hparams['k'], hparams['num_classes'], hparams['dropout']).to(hparams['device'])\n",
        "\n",
        "    accuracy = Accuracy(average='micro', compute_on_step=False).to(hparams['device'])\n",
        "    model.load_state_dict(torch.load(model_state_dict_root))\n",
        "    model.eval()\n",
        "    # Metric stored information reset:\n",
        "    accuracy.reset()\n",
        "    # Batch loop for validation:\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(test_loader, 1):\n",
        "            # Data retrieval from each bath:\n",
        "            points = to_dense_batch(data.pos, batch=data.batch)[0].to(hparams['device']).float().transpose(1, 2)\n",
        "            targets = data.y.to(hparams['device'])\n",
        "\n",
        "            # Forward pass:\n",
        "            preds, probs = model(points)\n",
        "\n",
        "            # Batch metrics calculation:\n",
        "            accuracy.update(probs, targets)\n",
        "\n",
        "    mean_accu = accuracy.compute().item()\n",
        "    # Print of all metrics:\n",
        "    print(\"Test Acc.: \", mean_accu)\n",
        "    return mean_accu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Owzqu_US1Z4D"
      },
      "source": [
        "# Experiments\n",
        "More info in the README."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ss3bj3reF3zM"
      },
      "source": [
        "def execute_experiment():\n",
        "  train_dataset, valid_dataset, test_dataset = get_train_valid_test_ModelNet(hparams['drive_root'], hparams['fixed_num_of_points'], hparams['normalize_scale'])\n",
        "  get_data_augmentation(train_dataset, hparams['data_augmentation'], hparams['normalize_scale'], hparams['flip_axis'], hparams['flip_probability'],\n",
        "                        hparams['rotate_axis'], hparams['rotate_degrees'])\n",
        "  best_acc, state_dict_root = fit(train_dataset, valid_dataset, hparams['num_classes'], hparams['k'], hparams['bs'],\n",
        "                                  hparams['epochs'], hparams['lr'])\n",
        "\n",
        "  test_inference = test(test_dataset, state_dict_root)\n",
        "\n",
        "def get_model_name(model, normalize_scale, data_augmentation, dropout, optimizer, scheduler):\n",
        "  model_name = model\n",
        "  model_name += '_normalized' if normalize_scale else '_notNormalized'\n",
        "  model_name += \"_\" + data_augmentation if data_augmentation is not None else '_noDataAugmentation'\n",
        "  model_name += \"_\" + str(dropout) if dropout is not None else '_noDropout'\n",
        "  model_name += \"_\" + optimizer\n",
        "  model_name += \"_\" + scheduler if scheduler is not None else \"_noScheduler\"\n",
        "  return model_name"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ONJsOIu1hMs"
      },
      "source": [
        "# Experiment 1\n",
        "hparams['optimizer'] = 'Adam'\n",
        "hparams['normalize_scale'] = False\n",
        "hparams['dropout'] = None\n",
        "hparams['scheduler'] = None\n",
        "hparams['data_augmentation'] = None\n",
        "hparams['epochs'] = 100\n",
        "hparams['model_name'] = get_model_name('pointNet', hparams['normalize_scale'], hparams['data_augmentation'], hparams['dropout'], hparams['optimizer'], hparams['scheduler'])\n",
        "execute_experiment()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gwV1u-s_lEt"
      },
      "source": [
        "# Experiment 2\n",
        "hparams['optimizer'] = 'Adam'\n",
        "hparams['normalize_scale'] = True\n",
        "hparams['dropout'] = None\n",
        "hparams['scheduler'] = None\n",
        "hparams['data_augmentation'] = None\n",
        "hparams['epochs'] = 100\n",
        "hparams['model_name'] = get_model_name('pointNet', hparams['normalize_scale'], hparams['data_augmentation'], hparams['dropout'], hparams['optimizer'], hparams['scheduler'])\n",
        "execute_experiment()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F65D_0KrGDHu"
      },
      "source": [
        "# Experiment 3\n",
        "hparams['optimizer'] = 'Adam'\n",
        "hparams['normalize_scale'] = True\n",
        "hparams['dropout'] = 0.3\n",
        "hparams['scheduler'] = None\n",
        "hparams['data_augmentation'] = None\n",
        "hparams['epochs'] = 100\n",
        "hparams['model_name'] = get_model_name('pointNet', hparams['normalize_scale'], hparams['data_augmentation'], hparams['dropout'], hparams['optimizer'], hparams['scheduler'])\n",
        "execute_experiment()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGOetxzLGcLX"
      },
      "source": [
        "# Experiment 4\n",
        "hparams['optimizer'] = 'Adam'\n",
        "hparams['normalize_scale'] = True\n",
        "hparams['dropout'] = 0.3\n",
        "hparams['scheduler'] = None\n",
        "hparams['data_augmentation'] = 'flip'\n",
        "hparams['epochs'] = 100 \n",
        "hparams['model_name'] = get_model_name('pointNet', hparams['normalize_scale'], hparams['data_augmentation'], hparams['dropout'], hparams['optimizer'], hparams['scheduler'])\n",
        "execute_experiment()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlC7YIzYGkRs"
      },
      "source": [
        "# Experiment 5\n",
        "hparams['optimizer'] = 'Adam'\n",
        "hparams['normalize_scale'] = True\n",
        "hparams['dropout'] = 0.3\n",
        "hparams['scheduler'] = None\n",
        "hparams['data_augmentation'] = 'rotate'\n",
        "hparams['epochs'] = 100\n",
        "hparams['model_name'] = get_model_name('pointNet', hparams['normalize_scale'], hparams['data_augmentation'], hparams['dropout'], hparams['optimizer'], hparams['scheduler'])\n",
        "execute_experiment()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNbTPKcEGujV"
      },
      "source": [
        "# Experiment 6\n",
        "hparams['optimizer'] = 'Adam'\n",
        "hparams['normalize_scale'] = True\n",
        "hparams['dropout'] = 0.3\n",
        "hparams['scheduler'] = None\n",
        "hparams['data_augmentation'] = 'flip_rotation'\n",
        "hparams['epochs'] = 100\n",
        "hparams['model_name'] = get_model_name('pointNet', hparams['normalize_scale'], hparams['data_augmentation'], hparams['dropout'], hparams['optimizer'], hparams['scheduler'])\n",
        "execute_experiment()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMiJWvNHb7Gl"
      },
      "source": [
        "# Experiment 7\n",
        "hparams['optimizer'] = 'Adam'\n",
        "hparams['normalize_scale'] = True\n",
        "hparams['dropout'] = 0.3\n",
        "hparams['scheduler'] = 'StepLR'\n",
        "hparams['data_augmentation'] = 'flip'\n",
        "hparams['epochs'] = 100\n",
        "hparams['model_name'] = get_model_name('pointNet', hparams['normalize_scale'], hparams['data_augmentation'], hparams['dropout'], hparams['optimizer'], hparams['scheduler'])\n",
        "execute_experiment()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNkTj8XNcPOk"
      },
      "source": [
        "# Experiment 8\n",
        "hparams['optimizer'] = 'Adam'\n",
        "hparams['normalize_scale'] = True\n",
        "hparams['dropout'] = 0.3\n",
        "hparams['scheduler'] = 'StepLR'\n",
        "hparams['data_augmentation'] = 'flip_rotation'\n",
        "hparams['epochs'] = 100\n",
        "hparams['model_name'] = get_model_name('pointNet', hparams['normalize_scale'], hparams['data_augmentation'], hparams['dropout'], hparams['optimizer'], hparams['scheduler'])\n",
        "execute_experiment()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHj3KwU-cTYU"
      },
      "source": [
        "# Experiment 9\n",
        "hparams['optimizer'] = 'Adam'\n",
        "hparams['normalize_scale'] = True\n",
        "hparams['dropout'] = 0.3\n",
        "hparams['scheduler'] = 'OneCycleLR'\n",
        "hparams['data_augmentation'] = 'flip'\n",
        "hparams['epochs'] = 100\n",
        "hparams['model_name'] = get_model_name('pointNet', hparams['normalize_scale'], hparams['data_augmentation'], hparams['dropout'], hparams['optimizer'], hparams['scheduler'])\n",
        "execute_experiment()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JExSSg99cfFV"
      },
      "source": [
        "# Experiment 10\n",
        "hparams['optimizer'] = 'Adam'\n",
        "hparams['normalize_scale'] = True\n",
        "hparams['dropout'] = 0.3\n",
        "hparams['scheduler'] = 'OneCycleLR'\n",
        "hparams['data_augmentation'] = 'flip_rotation'\n",
        "hparams['epochs'] = 100\n",
        "hparams['model_name'] = get_model_name('pointNet', hparams['normalize_scale'], hparams['data_augmentation'], hparams['dropout'], hparams['optimizer'], hparams['scheduler'])\n",
        "execute_experiment()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6ox66yacmb9"
      },
      "source": [
        "# Experiment 11\n",
        "hparams['optimizer'] = 'SGD'\n",
        "hparams['normalize_scale'] = True\n",
        "hparams['dropout'] = 0.3\n",
        "hparams['scheduler'] = None\n",
        "hparams['data_augmentation'] = 'flip_rotation'\n",
        "hparams['epochs'] = 100\n",
        "hparams['model_name'] = get_model_name('pointNet', hparams['normalize_scale'], hparams['data_augmentation'], hparams['dropout'], hparams['optimizer'], hparams['scheduler'])\n",
        "execute_experiment()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgnN3EHJc2-s"
      },
      "source": [
        "# Experiment 12\n",
        "hparams['optimizer'] = 'SGD'\n",
        "hparams['normalize_scale'] = True\n",
        "hparams['dropout'] = 0.3\n",
        "hparams['scheduler'] = 'StepLR'\n",
        "hparams['data_augmentation'] = 'flip_rotation'\n",
        "hparams['epochs'] = 100\n",
        "hparams['model_name'] = get_model_name('pointNet', hparams['normalize_scale'], hparams['data_augmentation'], hparams['dropout'], hparams['optimizer'], hparams['scheduler'])\n",
        "execute_experiment()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCost2GDc838"
      },
      "source": [
        "# Experiment 13\n",
        "hparams['optimizer'] = 'SGD'\n",
        "hparams['normalize_scale'] = True\n",
        "hparams['dropout'] = 0.3\n",
        "hparams['scheduler'] = 'OneCycleLR'\n",
        "hparams['data_augmentation'] = 'flip_rotation'\n",
        "hparams['epochs'] = 100\n",
        "hparams['model_name'] = get_model_name('pointNet', hparams['normalize_scale'], hparams['data_augmentation'], hparams['dropout'], hparams['optimizer'], hparams['scheduler'])\n",
        "execute_experiment()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Em_fX6TDdCzU"
      },
      "source": [
        "# Experiment 14\n",
        "hparams['optimizer'] = 'SGD'\n",
        "hparams['normalize_scale'] = True\n",
        "hparams['dropout'] = 0.3\n",
        "hparams['scheduler'] = None\n",
        "hparams['data_augmentation'] = 'flip'\n",
        "hparams['epochs'] = 100\n",
        "hparams['model_name'] = get_model_name('pointNet', hparams['normalize_scale'], hparams['data_augmentation'], hparams['dropout'], hparams['optimizer'], hparams['scheduler'])\n",
        "execute_experiment()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGCTvmkNdLbL"
      },
      "source": [
        "# Experiment 15\n",
        "hparams['optimizer'] = 'SGD'\n",
        "hparams['normalize_scale'] = True\n",
        "hparams['dropout'] = 0.3\n",
        "hparams['scheduler'] = 'StepLR'\n",
        "hparams['data_augmentation'] = 'flip'\n",
        "hparams['epochs'] = 100\n",
        "hparams['model_name'] = get_model_name('pointNet', hparams['normalize_scale'], hparams['data_augmentation'], hparams['dropout'], hparams['optimizer'], hparams['scheduler'])\n",
        "execute_experiment()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Frezi74YdPur"
      },
      "source": [
        "# Experiment 16\n",
        "hparams['optimizer'] = 'SGD'\n",
        "hparams['normalize_scale'] = True\n",
        "hparams['dropout'] = 0.3\n",
        "hparams['scheduler'] = 'OneCycleLR'\n",
        "hparams['data_augmentation'] = 'flip'\n",
        "hparams['epochs'] = 100\n",
        "hparams['model_name'] = get_model_name('pointNet', hparams['normalize_scale'], hparams['data_augmentation'], hparams['dropout'], hparams['optimizer'], hparams['scheduler'])\n",
        "execute_experiment()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}